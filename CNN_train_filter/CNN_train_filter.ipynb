{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn3xf_c2qIe2",
        "outputId": "dfa59896-60f2-491f-f539-c2b7959c3382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Matrix:\n",
            "[[ 1.  2.  3.  4.  5.]\n",
            " [ 6.  7.  8.  9. 10.]\n",
            " [11. 12. 13. 14. 15.]\n",
            " [16. 17. 18. 19. 20.]\n",
            " [21. 22. 23. 24. 25.]]\n",
            "\n",
            "Filter Weights:\n",
            "[[ 1.  0. -1.]\n",
            " [ 2.  0. -2.]\n",
            " [ 1.  0. -1.]]\n",
            "\n",
            "Convolution Result:\n",
            "[[-8. -8. -8.]\n",
            " [-8. -8. -8.]\n",
            " [-8. -8. -8.]]\n",
            "\n",
            "Target Matrix:\n",
            "[[10. 10. 10.]\n",
            " [10. 10. 10.]\n",
            " [10. 10. 10.]]\n",
            "\n",
            "Error:\n",
            "[[18. 18. 18.]\n",
            " [18. 18. 18.]\n",
            " [18. 18. 18.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a 5x5 matrix as input\n",
        "input_matrix = np.array([\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [6, 7, 8, 9, 10],\n",
        "    [11, 12, 13, 14, 15],\n",
        "    [16, 17, 18, 19, 20],\n",
        "    [21, 22, 23, 24, 25]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Define a 3x3 filter\n",
        "filter_weights = np.array([\n",
        "    [1, 0, -1],\n",
        "    [2, 0, -2],\n",
        "    [1, 0, -1]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Calculate the convolution result (valid padding)\n",
        "def convolution(input_matrix, filter_weights):\n",
        "    input_height, input_width = input_matrix.shape\n",
        "    filter_height, filter_width = filter_weights.shape\n",
        "    output_height = input_height - filter_height + 1\n",
        "    output_width = input_width - filter_width + 1\n",
        "    output_matrix = np.zeros((output_height, output_width), dtype=np.float32)\n",
        "\n",
        "    for i in range(output_height):\n",
        "        for j in range(output_width):\n",
        "            output_matrix[i, j] = np.sum(input_matrix[i:i+filter_height, j:j+filter_width] * filter_weights)\n",
        "\n",
        "    return output_matrix\n",
        "\n",
        "# Perform convolution\n",
        "conv_result = convolution(input_matrix, filter_weights)\n",
        "\n",
        "# Define a target matrix (gradients with respect to the input)\n",
        "target_matrix = np.array([\n",
        "    [10, 10, 10],\n",
        "    [10, 10, 10],\n",
        "    [10, 10, 10]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Calculate the error (difference between the convolution result and the target)\n",
        "error = target_matrix - conv_result\n",
        "\n",
        "# Print the results\n",
        "print(\"Input Matrix:\")\n",
        "print(input_matrix)\n",
        "print(\"\\nFilter Weights:\")\n",
        "print(filter_weights)\n",
        "print(\"\\nConvolution Result:\")\n",
        "print(conv_result)\n",
        "print(\"\\nTarget Matrix:\")\n",
        "print(target_matrix)\n",
        "print(\"\\nError:\")\n",
        "print(error)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV4jaMPIq6qB",
        "outputId": "02bea9a5-70fb-4035-ab63-c6ffa8426270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.04 0.08 0.12 0.16 0.2 ]\n",
            " [0.24 0.28 0.32 0.36 0.4 ]\n",
            " [0.44 0.48 0.52 0.56 0.6 ]\n",
            " [0.64 0.68 0.72 0.76 0.8 ]\n",
            " [0.84 0.88 0.92 0.96 1.  ]]\n",
            "Epoch 0, Loss: 958.5215454101562\n",
            "Epoch 1000, Loss: 51.490318298339844\n",
            "Epoch 2000, Loss: 35.029136657714844\n",
            "Epoch 3000, Loss: 23.830516815185547\n",
            "Epoch 4000, Loss: 16.212013244628906\n",
            "Epoch 5000, Loss: 11.029134750366211\n",
            "Epoch 6000, Loss: 7.503177642822266\n",
            "Epoch 7000, Loss: 5.104464054107666\n",
            "Epoch 8000, Loss: 3.472597360610962\n",
            "Epoch 9000, Loss: 2.362431049346924\n",
            "Epoch 10000, Loss: 1.6071748733520508\n",
            "Epoch 11000, Loss: 1.0933741331100464\n",
            "Epoch 12000, Loss: 0.7438321709632874\n",
            "Epoch 13000, Loss: 0.5060402154922485\n",
            "Epoch 14000, Loss: 0.34426945447921753\n",
            "Epoch 15000, Loss: 0.2342129349708557\n",
            "Epoch 16000, Loss: 0.1593412458896637\n",
            "Epoch 17000, Loss: 0.10841048508882523\n",
            "Epoch 18000, Loss: 0.07376018911600113\n",
            "Epoch 19000, Loss: 0.05018775537610054\n",
            "Epoch 20000, Loss: 0.034149982035160065\n",
            "Epoch 21000, Loss: 0.023239534348249435\n",
            "Epoch 22000, Loss: 0.015816112980246544\n",
            "Epoch 23000, Loss: 0.01076375413686037\n",
            "Epoch 24000, Loss: 0.007327336817979813\n",
            "Epoch 25000, Loss: 0.004989369306713343\n",
            "Epoch 26000, Loss: 0.003398149274289608\n",
            "Epoch 27000, Loss: 0.002313230885192752\n",
            "Epoch 28000, Loss: 0.0015757448272779584\n",
            "Epoch 29000, Loss: 0.0010719657875597477\n",
            "Epoch 30000, Loss: 0.0007315575494430959\n",
            "Epoch 31000, Loss: 0.0004992580506950617\n",
            "Epoch 32000, Loss: 0.0003398956614546478\n",
            "Epoch 33000, Loss: 0.00023209357459563762\n",
            "Epoch 34000, Loss: 0.00015887265908531845\n",
            "Epoch 35000, Loss: 0.0001078579225577414\n",
            "Epoch 36000, Loss: 7.433966675307602e-05\n",
            "Epoch 37000, Loss: 5.025505743105896e-05\n",
            "Epoch 38000, Loss: 3.4425785997882485e-05\n",
            "Epoch 39000, Loss: 2.373605821048841e-05\n",
            "Epoch 40000, Loss: 1.5837085811654106e-05\n",
            "Epoch 41000, Loss: 1.0455905794515274e-05\n",
            "Epoch 42000, Loss: 6.351193405862432e-06\n",
            "Epoch 43000, Loss: 4.500217983149923e-06\n",
            "Epoch 44000, Loss: 3.865046892315149e-06\n",
            "Epoch 45000, Loss: 3.3967753552133217e-06\n",
            "Epoch 46000, Loss: 2.9738075681962073e-06\n",
            "Epoch 47000, Loss: 2.6337993404013105e-06\n",
            "Epoch 48000, Loss: 2.6093903215951286e-06\n",
            "Epoch 49000, Loss: 2.5936788006220013e-06\n",
            "Epoch 50000, Loss: 2.5761437427718192e-06\n",
            "Epoch 51000, Loss: 2.5616882339818403e-06\n",
            "Epoch 52000, Loss: 2.5410972739337012e-06\n",
            "Epoch 53000, Loss: 2.527051947254222e-06\n",
            "Epoch 54000, Loss: 2.5103554435190745e-06\n",
            "Epoch 55000, Loss: 2.496250090189278e-06\n",
            "Epoch 56000, Loss: 2.4827904780977406e-06\n",
            "Epoch 57000, Loss: 2.4689925339771435e-06\n",
            "Epoch 58000, Loss: 2.455228241160512e-06\n",
            "Epoch 59000, Loss: 2.4427854441455565e-06\n",
            "Epoch 60000, Loss: 2.4275741452584043e-06\n",
            "Epoch 61000, Loss: 2.4115161068039015e-06\n",
            "Epoch 62000, Loss: 2.399938239250332e-06\n",
            "Epoch 63000, Loss: 2.3894317564554513e-06\n",
            "Epoch 64000, Loss: 2.3743141355225816e-06\n",
            "Epoch 65000, Loss: 2.360967300774064e-06\n",
            "Epoch 66000, Loss: 2.348983798583504e-06\n",
            "Epoch 67000, Loss: 2.3361517378361896e-06\n",
            "Epoch 68000, Loss: 2.319359737157356e-06\n",
            "Epoch 69000, Loss: 2.308821422047913e-06\n",
            "Epoch 70000, Loss: 2.2950662241782993e-06\n",
            "Epoch 71000, Loss: 2.2803224055678584e-06\n",
            "Epoch 72000, Loss: 2.267171112180222e-06\n",
            "Epoch 73000, Loss: 2.254922947031446e-06\n",
            "Epoch 74000, Loss: 2.240826688648667e-06\n",
            "Epoch 75000, Loss: 2.227133336418774e-06\n",
            "Epoch 76000, Loss: 2.2168578652781434e-06\n",
            "Epoch 77000, Loss: 2.205901182605885e-06\n",
            "Epoch 78000, Loss: 2.191245584981516e-06\n",
            "Epoch 79000, Loss: 2.177195710828528e-06\n",
            "Epoch 80000, Loss: 2.16454645851627e-06\n",
            "Epoch 81000, Loss: 2.153155037376564e-06\n",
            "Epoch 82000, Loss: 2.1383129933383316e-06\n",
            "Epoch 83000, Loss: 2.1245859898044728e-06\n",
            "Epoch 84000, Loss: 2.113430127792526e-06\n",
            "Epoch 85000, Loss: 2.1008399926358834e-06\n",
            "Epoch 86000, Loss: 2.087997017952148e-06\n",
            "Epoch 87000, Loss: 2.075899828923866e-06\n",
            "Epoch 88000, Loss: 2.0657098502852023e-06\n",
            "Epoch 89000, Loss: 2.055477125395555e-06\n",
            "Epoch 90000, Loss: 2.0407032934599556e-06\n",
            "Epoch 91000, Loss: 2.0275665519875474e-06\n",
            "Epoch 92000, Loss: 2.0159177438472398e-06\n",
            "Epoch 93000, Loss: 2.004369889618829e-06\n",
            "Epoch 94000, Loss: 1.990527380257845e-06\n",
            "Epoch 95000, Loss: 1.97796180145815e-06\n",
            "Epoch 96000, Loss: 1.9660492398543283e-06\n",
            "Epoch 97000, Loss: 1.9556664483388886e-06\n",
            "Epoch 98000, Loss: 1.945098119904287e-06\n",
            "Epoch 99000, Loss: 1.9313947632326744e-06\n",
            "Epoch 100000, Loss: 1.9201606846763752e-06\n",
            "Epoch 101000, Loss: 1.909495949803386e-06\n",
            "Epoch 102000, Loss: 1.8969903976540081e-06\n",
            "Epoch 103000, Loss: 1.8840601114789024e-06\n",
            "Epoch 104000, Loss: 1.8723640096141025e-06\n",
            "Epoch 105000, Loss: 1.8613663996802643e-06\n",
            "Epoch 106000, Loss: 1.8507389540900476e-06\n",
            "Epoch 107000, Loss: 1.8401306078885682e-06\n",
            "Epoch 108000, Loss: 1.8278524294146337e-06\n",
            "Epoch 109000, Loss: 1.8165983419748954e-06\n",
            "Epoch 110000, Loss: 1.8064856703858823e-06\n",
            "Epoch 111000, Loss: 1.79270045919111e-06\n",
            "Epoch 112000, Loss: 1.7801257854443975e-06\n",
            "Epoch 113000, Loss: 1.7703741832519881e-06\n",
            "Epoch 114000, Loss: 1.7595539247849956e-06\n",
            "Epoch 115000, Loss: 1.7496658983873203e-06\n",
            "Epoch 116000, Loss: 1.7385864339303225e-06\n",
            "Epoch 117000, Loss: 1.7264628695556894e-06\n",
            "Epoch 118000, Loss: 1.7175598259200342e-06\n",
            "Epoch 119000, Loss: 1.7057627701433375e-06\n",
            "Epoch 120000, Loss: 1.6948888514889404e-06\n",
            "Epoch 121000, Loss: 1.6829817468533292e-06\n",
            "Epoch 122000, Loss: 1.6725598470657133e-06\n",
            "Epoch 123000, Loss: 1.662543581915088e-06\n",
            "Epoch 124000, Loss: 1.6534904716536403e-06\n",
            "Epoch 125000, Loss: 1.6421636246377602e-06\n",
            "Epoch 126000, Loss: 1.6329859136021696e-06\n",
            "Epoch 127000, Loss: 1.6188259905902669e-06\n",
            "Epoch 128000, Loss: 1.6089752534753643e-06\n",
            "Epoch 129000, Loss: 1.5977984730852768e-06\n",
            "Epoch 130000, Loss: 1.5892319424892776e-06\n",
            "Epoch 131000, Loss: 1.5800760593265295e-06\n",
            "Epoch 132000, Loss: 1.5687019185861573e-06\n",
            "Epoch 133000, Loss: 1.5587611414957792e-06\n",
            "Epoch 134000, Loss: 1.5474633983103558e-06\n",
            "Epoch 135000, Loss: 1.5379509932245128e-06\n",
            "Epoch 136000, Loss: 1.530327608634252e-06\n",
            "Epoch 137000, Loss: 1.5191908460110426e-06\n",
            "Epoch 138000, Loss: 1.5118503142730333e-06\n",
            "Epoch 139000, Loss: 1.503576640971005e-06\n",
            "Epoch 140000, Loss: 1.494259777246043e-06\n",
            "Epoch 141000, Loss: 1.487451299908571e-06\n",
            "Epoch 142000, Loss: 1.4775350791751407e-06\n",
            "Epoch 143000, Loss: 1.471709765610285e-06\n",
            "Epoch 144000, Loss: 1.4663610272691585e-06\n",
            "Epoch 145000, Loss: 1.4575098248315044e-06\n",
            "Epoch 146000, Loss: 1.4530623957398348e-06\n",
            "Epoch 147000, Loss: 1.4481638572760858e-06\n",
            "Epoch 148000, Loss: 1.4416355043067597e-06\n",
            "Epoch 149000, Loss: 1.4334973457152955e-06\n",
            "\n",
            "Final Filter Weights:\n",
            "[[-8.9210424e+00 -8.2667074e+00 -7.6152811e+00]\n",
            " [ 3.4082368e-01  9.7440585e-05 -3.4005713e-01]\n",
            " [ 7.6164761e+00  8.2664061e+00  8.9215631e+00]]\n",
            "\n",
            "Original Input Matrix:\n",
            "[[0.04 0.08 0.12 0.16 0.2 ]\n",
            " [0.24 0.28 0.32 0.36 0.4 ]\n",
            " [0.44 0.48 0.52 0.56 0.6 ]\n",
            " [0.64 0.68 0.72 0.76 0.8 ]\n",
            " [0.84 0.88 0.92 0.96 1.  ]]\n",
            "\n",
            "Convolution Result with Trained Filter on Original Input Matrix:\n",
            "[[ 9.999331   9.999422   9.999514 ]\n",
            " [ 9.999788   9.99988    9.9999695]\n",
            " [10.000244  10.000335  10.000425 ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a 5x5 matrix as input\n",
        "input_matrix = np.array([\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [6, 7, 8, 9, 10],\n",
        "    [11, 12, 13, 14, 15],\n",
        "    [16, 17, 18, 19, 20],\n",
        "    [21, 22, 23, 24, 25]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Define a 3x3 filter\n",
        "filter_weights = np.array([\n",
        "    [1, 0, -1],\n",
        "    [2, 0, -2],\n",
        "    [1, 0, -1]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Define a target matrix (gradients with respect to the input)\n",
        "target_matrix = np.array([\n",
        "    [10, 10, 10],\n",
        "    [10, 10, 10],\n",
        "    [10, 10, 10]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "epochs = 150000\n",
        "\n",
        "input_matrix/= np.max(np.abs(input_matrix))\n",
        "\n",
        "print(input_matrix)\n",
        "\n",
        "# Perform gradient descent\n",
        "for epoch in range(epochs):\n",
        "    # Perform convolution\n",
        "    conv_result = np.zeros_like(target_matrix, dtype=np.float32)\n",
        "    for i in range(conv_result.shape[0]):\n",
        "        for j in range(conv_result.shape[1]):\n",
        "            conv_result[i, j] = np.sum(input_matrix[i:i+filter_weights.shape[0], j:j+filter_weights.shape[1]] * filter_weights)\n",
        "\n",
        "    # Calculate the error\n",
        "    error = target_matrix - conv_result\n",
        "    # print(error)\n",
        "    # Calculate the gradient\n",
        "    gradient = np.zeros_like(filter_weights, dtype=np.float32)\n",
        "    listGradient=[]\n",
        "    for i in range(gradient.shape[0]):\n",
        "        for j in range(gradient.shape[1]):\n",
        "            e = error[i,j]\n",
        "            # print(e)\n",
        "            gradient = e * -1 * 1 * input_matrix[i:i+filter_weights.shape[0], j:j+filter_weights.shape[1]]\n",
        "            # gradient /= np.max(np.abs(gradient))\n",
        "            listGradient.append(gradient)\n",
        "            # print(gradient)\n",
        "\n",
        "    # Normalize the gradient\n",
        "    # gradient /= np.max(np.abs(gradient))\n",
        "    # gradient /= 16.0\n",
        "\n",
        "    gradient=np.sum(listGradient, axis=0)\n",
        "    # print(gradient)\n",
        "    # print(10*'**********')\n",
        "    # 1/0\n",
        "    # Update the filter weights using gradient descent\n",
        "    filter_weights = filter_weights - learning_rate * gradient\n",
        "\n",
        "    # Print the loss every 10 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.sum(error ** 2)\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Print the final filter weights\n",
        "print(\"\\nFinal Filter Weights:\")\n",
        "print(filter_weights)\n",
        "\n",
        "# Perform convolution with the trained filter on the original input_matrix\n",
        "conv_result_original = np.zeros_like(target_matrix, dtype=np.float32)\n",
        "for i in range(conv_result_original.shape[0]):\n",
        "    for j in range(conv_result_original.shape[1]):\n",
        "        conv_result_original[i, j] = np.sum(input_matrix[i:i+filter_weights.shape[0], j:j+filter_weights.shape[1]] * filter_weights)\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nOriginal Input Matrix:\")\n",
        "print(input_matrix)\n",
        "print(\"\\nConvolution Result with Trained Filter on Original Input Matrix:\")\n",
        "print(conv_result_original)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UzuhuEG_L1ji",
        "outputId": "eb6f5753-2e2b-4534-c5f2-7794b4b7ceee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 1600.0\n",
            "Epoch 100, Loss: 168.84518432617188\n",
            "Epoch 200, Loss: 219.87356567382812\n",
            "Epoch 300, Loss: 233.87246704101562\n",
            "Epoch 400, Loss: 231.31405639648438\n",
            "Epoch 500, Loss: 225.63816833496094\n",
            "Epoch 600, Loss: 219.5428466796875\n",
            "Epoch 700, Loss: 213.51287841796875\n",
            "Epoch 800, Loss: 207.63111877441406\n",
            "Epoch 900, Loss: 201.90798950195312\n",
            "Epoch 1000, Loss: 196.34234619140625\n",
            "Epoch 1100, Loss: 190.92984008789062\n",
            "Epoch 1200, Loss: 185.66651916503906\n",
            "Epoch 1300, Loss: 180.54837036132812\n",
            "Epoch 1400, Loss: 175.57130432128906\n",
            "Epoch 1500, Loss: 170.73141479492188\n",
            "Epoch 1600, Loss: 166.0249481201172\n",
            "Epoch 1700, Loss: 161.4482421875\n",
            "Epoch 1800, Loss: 156.9976806640625\n",
            "Epoch 1900, Loss: 152.66978454589844\n",
            "Epoch 2000, Loss: 148.46124267578125\n",
            "Epoch 2100, Loss: 144.36868286132812\n",
            "Epoch 2200, Loss: 140.3889617919922\n",
            "Epoch 2300, Loss: 136.51890563964844\n",
            "Epoch 2400, Loss: 132.75555419921875\n",
            "Epoch 2500, Loss: 129.09591674804688\n",
            "Epoch 2600, Loss: 125.53723907470703\n",
            "Epoch 2700, Loss: 122.07666778564453\n",
            "Epoch 2800, Loss: 118.71145629882812\n",
            "Epoch 2900, Loss: 115.4389419555664\n",
            "Epoch 3000, Loss: 112.25675964355469\n",
            "Epoch 3100, Loss: 109.16220092773438\n",
            "Epoch 3200, Loss: 106.15299987792969\n",
            "Epoch 3300, Loss: 103.22672271728516\n",
            "Epoch 3400, Loss: 100.38108825683594\n",
            "Epoch 3500, Loss: 97.61388397216797\n",
            "Epoch 3600, Loss: 94.92298889160156\n",
            "Epoch 3700, Loss: 92.30633544921875\n",
            "Epoch 3800, Loss: 89.76177978515625\n",
            "Epoch 3900, Loss: 87.28742218017578\n",
            "Epoch 4000, Loss: 84.88117980957031\n",
            "Epoch 4100, Loss: 82.54130554199219\n",
            "Epoch 4200, Loss: 80.26593017578125\n",
            "Epoch 4300, Loss: 78.05328369140625\n",
            "Epoch 4400, Loss: 75.90160369873047\n",
            "Epoch 4500, Loss: 73.80931091308594\n",
            "Epoch 4600, Loss: 71.774658203125\n",
            "Epoch 4700, Loss: 69.79608154296875\n",
            "Epoch 4800, Loss: 67.87203979492188\n",
            "Epoch 4900, Loss: 66.00108337402344\n",
            "Epoch 5000, Loss: 64.18161010742188\n",
            "Epoch 5100, Loss: 62.4123420715332\n",
            "Epoch 5200, Loss: 60.691856384277344\n",
            "Epoch 5300, Loss: 59.0188102722168\n",
            "Epoch 5400, Loss: 57.39185333251953\n",
            "Epoch 5500, Loss: 55.80976867675781\n",
            "Epoch 5600, Loss: 54.27129364013672\n",
            "Epoch 5700, Loss: 52.775238037109375\n",
            "Epoch 5800, Loss: 51.320343017578125\n",
            "Epoch 5900, Loss: 49.90569305419922\n",
            "Epoch 6000, Loss: 48.52996826171875\n",
            "Epoch 6100, Loss: 47.19218444824219\n",
            "Epoch 6200, Loss: 45.891265869140625\n",
            "Epoch 6300, Loss: 44.62617111206055\n",
            "Epoch 6400, Loss: 43.39598083496094\n",
            "Epoch 6500, Loss: 42.19969940185547\n",
            "Epoch 6600, Loss: 41.03641891479492\n",
            "Epoch 6700, Loss: 39.90519332885742\n",
            "Epoch 6800, Loss: 38.805145263671875\n",
            "Epoch 6900, Loss: 37.73539733886719\n",
            "Epoch 7000, Loss: 36.6951904296875\n",
            "Epoch 7100, Loss: 35.683624267578125\n",
            "Epoch 7200, Loss: 34.69997024536133\n",
            "Epoch 7300, Loss: 33.74342346191406\n",
            "Epoch 7400, Loss: 32.81324005126953\n",
            "Epoch 7500, Loss: 31.908702850341797\n",
            "Epoch 7600, Loss: 31.029090881347656\n",
            "Epoch 7700, Loss: 30.1737117767334\n",
            "Epoch 7800, Loss: 29.34192657470703\n",
            "Epoch 7900, Loss: 28.533077239990234\n",
            "Epoch 8000, Loss: 27.746522903442383\n",
            "Epoch 8100, Loss: 26.981651306152344\n",
            "Epoch 8200, Loss: 26.237869262695312\n",
            "Epoch 8300, Loss: 25.514598846435547\n",
            "Epoch 8400, Loss: 24.811222076416016\n",
            "Epoch 8500, Loss: 24.127281188964844\n",
            "Epoch 8600, Loss: 23.462200164794922\n",
            "Epoch 8700, Loss: 22.815404891967773\n",
            "Epoch 8800, Loss: 22.186481475830078\n",
            "Epoch 8900, Loss: 21.57487678527832\n",
            "Epoch 9000, Loss: 20.98016357421875\n",
            "Epoch 9100, Loss: 20.40178871154785\n",
            "Epoch 9200, Loss: 19.839387893676758\n",
            "Epoch 9300, Loss: 19.292495727539062\n",
            "Epoch 9400, Loss: 18.760665893554688\n",
            "Epoch 9500, Loss: 18.243446350097656\n",
            "Epoch 9600, Loss: 17.740602493286133\n",
            "Epoch 9700, Loss: 17.25153923034668\n",
            "Epoch 9800, Loss: 16.775955200195312\n",
            "Epoch 9900, Loss: 16.313573837280273\n",
            "Epoch 10000, Loss: 15.863805770874023\n",
            "Epoch 10100, Loss: 15.426536560058594\n",
            "Epoch 10200, Loss: 15.00126838684082\n",
            "Epoch 10300, Loss: 14.587703704833984\n",
            "Epoch 10400, Loss: 14.185653686523438\n",
            "Epoch 10500, Loss: 13.794536590576172\n",
            "Epoch 10600, Loss: 13.414304733276367\n",
            "Epoch 10700, Loss: 13.044463157653809\n",
            "Epoch 10800, Loss: 12.684940338134766\n",
            "Epoch 10900, Loss: 12.33525276184082\n",
            "Epoch 11000, Loss: 11.995224952697754\n",
            "Epoch 11100, Loss: 11.664567947387695\n",
            "Epoch 11200, Loss: 11.342977523803711\n",
            "Epoch 11300, Loss: 11.030277252197266\n",
            "Epoch 11400, Loss: 10.726247787475586\n",
            "Epoch 11500, Loss: 10.430584907531738\n",
            "Epoch 11600, Loss: 10.143051147460938\n",
            "Epoch 11700, Loss: 9.863441467285156\n",
            "Epoch 11800, Loss: 9.591512680053711\n",
            "Epoch 11900, Loss: 9.327136993408203\n",
            "Epoch 12000, Loss: 9.070028305053711\n",
            "Epoch 12100, Loss: 8.819968223571777\n",
            "Epoch 12200, Loss: 8.57686996459961\n",
            "Epoch 12300, Loss: 8.340410232543945\n",
            "Epoch 12400, Loss: 8.110499382019043\n",
            "Epoch 12500, Loss: 7.886904716491699\n",
            "Epoch 12600, Loss: 7.669553756713867\n",
            "Epoch 12700, Loss: 7.458091735839844\n",
            "Epoch 12800, Loss: 7.2525634765625\n",
            "Epoch 12900, Loss: 7.0526123046875\n",
            "Epoch 13000, Loss: 6.858159065246582\n",
            "Epoch 13100, Loss: 6.669144153594971\n",
            "Epoch 13200, Loss: 6.485332489013672\n",
            "Epoch 13300, Loss: 6.306534767150879\n",
            "Epoch 13400, Loss: 6.1326704025268555\n",
            "Epoch 13500, Loss: 5.963606834411621\n",
            "Epoch 13600, Loss: 5.799219131469727\n",
            "Epoch 13700, Loss: 5.639341831207275\n",
            "Epoch 13800, Loss: 5.483922958374023\n",
            "Epoch 13900, Loss: 5.332736968994141\n",
            "Epoch 14000, Loss: 5.185738563537598\n",
            "Epoch 14100, Loss: 5.04277229309082\n",
            "Epoch 14200, Loss: 4.903794288635254\n",
            "Epoch 14300, Loss: 4.768585205078125\n",
            "Epoch 14400, Loss: 4.637138366699219\n",
            "Epoch 14500, Loss: 4.509334564208984\n",
            "Epoch 14600, Loss: 4.3850507736206055\n",
            "Epoch 14700, Loss: 4.264125823974609\n",
            "Epoch 14800, Loss: 4.146615028381348\n",
            "Epoch 14900, Loss: 4.032280445098877\n",
            "Epoch 15000, Loss: 3.921130657196045\n",
            "Epoch 15100, Loss: 3.813103199005127\n",
            "Epoch 15200, Loss: 3.7079553604125977\n",
            "Epoch 15300, Loss: 3.6057300567626953\n",
            "Epoch 15400, Loss: 3.506389856338501\n",
            "Epoch 15500, Loss: 3.4097280502319336\n",
            "Epoch 15600, Loss: 3.3156991004943848\n",
            "Epoch 15700, Loss: 3.224285125732422\n",
            "Epoch 15800, Loss: 3.135423183441162\n",
            "Epoch 15900, Loss: 3.0490224361419678\n",
            "Epoch 16000, Loss: 2.9649252891540527\n",
            "Epoch 16100, Loss: 2.8831896781921387\n",
            "Epoch 16200, Loss: 2.8037142753601074\n",
            "Epoch 16300, Loss: 2.7264368534088135\n",
            "Epoch 16400, Loss: 2.651298761367798\n",
            "Epoch 16500, Loss: 2.5781893730163574\n",
            "Epoch 16600, Loss: 2.5072021484375\n",
            "Epoch 16700, Loss: 2.438042640686035\n",
            "Epoch 16800, Loss: 2.3708248138427734\n",
            "Epoch 16900, Loss: 2.3055384159088135\n",
            "Epoch 17000, Loss: 2.2419345378875732\n",
            "Epoch 17100, Loss: 2.180145502090454\n",
            "Epoch 17200, Loss: 2.120084285736084\n",
            "Epoch 17300, Loss: 2.061617374420166\n",
            "Epoch 17400, Loss: 2.004789352416992\n",
            "Epoch 17500, Loss: 1.9495069980621338\n",
            "Epoch 17600, Loss: 1.8957680463790894\n",
            "Epoch 17700, Loss: 1.84352707862854\n",
            "Epoch 17800, Loss: 1.7927007675170898\n",
            "Epoch 17900, Loss: 1.743283748626709\n",
            "Epoch 18000, Loss: 1.695197582244873\n",
            "Epoch 18100, Loss: 1.6484874486923218\n",
            "Epoch 18200, Loss: 1.6030594110488892\n",
            "Epoch 18300, Loss: 1.558904767036438\n",
            "Epoch 18400, Loss: 1.5159116983413696\n",
            "Epoch 18500, Loss: 1.4741361141204834\n",
            "Epoch 18600, Loss: 1.4334783554077148\n",
            "Epoch 18700, Loss: 1.393986701965332\n",
            "Epoch 18800, Loss: 1.355581283569336\n",
            "Epoch 18900, Loss: 1.3182342052459717\n",
            "Epoch 19000, Loss: 1.2818691730499268\n",
            "Epoch 19100, Loss: 1.246530532836914\n",
            "Epoch 19200, Loss: 1.212200403213501\n",
            "Epoch 19300, Loss: 1.1787607669830322\n",
            "Epoch 19400, Loss: 1.1462771892547607\n",
            "Epoch 19500, Loss: 1.1146653890609741\n",
            "Epoch 19600, Loss: 1.0839463472366333\n",
            "Epoch 19700, Loss: 1.0540536642074585\n",
            "Epoch 19800, Loss: 1.0249786376953125\n",
            "Epoch 19900, Loss: 0.9967285394668579\n",
            "Epoch 20000, Loss: 0.9692848324775696\n",
            "Epoch 20100, Loss: 0.9425797462463379\n",
            "Epoch 20200, Loss: 0.9166141152381897\n",
            "Epoch 20300, Loss: 0.8913326859474182\n",
            "Epoch 20400, Loss: 0.866688072681427\n",
            "Epoch 20500, Loss: 0.8428826332092285\n",
            "Epoch 20600, Loss: 0.8196446299552917\n",
            "Epoch 20700, Loss: 0.7970459461212158\n",
            "Epoch 20800, Loss: 0.7750924229621887\n",
            "Epoch 20900, Loss: 0.7536829710006714\n",
            "Epoch 21000, Loss: 0.7329420447349548\n",
            "Epoch 21100, Loss: 0.7127488851547241\n",
            "Epoch 21200, Loss: 0.6930924654006958\n",
            "Epoch 21300, Loss: 0.674012303352356\n",
            "Epoch 21400, Loss: 0.6554319262504578\n",
            "Epoch 21500, Loss: 0.6373727321624756\n",
            "Epoch 21600, Loss: 0.6197103261947632\n",
            "Epoch 21700, Loss: 0.6027113795280457\n",
            "Epoch 21800, Loss: 0.5860954523086548\n",
            "Epoch 21900, Loss: 0.5699720978736877\n",
            "Epoch 22000, Loss: 0.5542402863502502\n",
            "Epoch 22100, Loss: 0.5390046238899231\n",
            "Epoch 22200, Loss: 0.5240744352340698\n",
            "Epoch 22300, Loss: 0.5096963047981262\n",
            "Epoch 22400, Loss: 0.49561628699302673\n",
            "Epoch 22500, Loss: 0.4819602966308594\n",
            "Epoch 22600, Loss: 0.4686555862426758\n",
            "Epoch 22700, Loss: 0.45576420426368713\n",
            "Epoch 22800, Loss: 0.4432187080383301\n",
            "Epoch 22900, Loss: 0.4309867322444916\n",
            "Epoch 23000, Loss: 0.41908350586891174\n",
            "Epoch 23100, Loss: 0.4075673222541809\n",
            "Epoch 23200, Loss: 0.39633411169052124\n",
            "Epoch 23300, Loss: 0.3853989541530609\n",
            "Epoch 23400, Loss: 0.3748084604740143\n",
            "Epoch 23500, Loss: 0.36444640159606934\n",
            "Epoch 23600, Loss: 0.3543880581855774\n",
            "Epoch 23700, Loss: 0.34468579292297363\n",
            "Epoch 23800, Loss: 0.33511796593666077\n",
            "Epoch 23900, Loss: 0.3259581923484802\n",
            "Epoch 24000, Loss: 0.31688952445983887\n",
            "Epoch 24100, Loss: 0.3082001805305481\n",
            "Epoch 24200, Loss: 0.2997381091117859\n",
            "Epoch 24300, Loss: 0.2914271056652069\n",
            "Epoch 24400, Loss: 0.283428817987442\n",
            "Epoch 24500, Loss: 0.2756153345108032\n",
            "Epoch 24600, Loss: 0.2679808735847473\n",
            "Epoch 24700, Loss: 0.260644793510437\n",
            "Epoch 24800, Loss: 0.2534186840057373\n",
            "Epoch 24900, Loss: 0.24641095101833344\n",
            "Epoch 25000, Loss: 0.23968610167503357\n",
            "Epoch 25100, Loss: 0.2330714464187622\n",
            "Epoch 25200, Loss: 0.22664666175842285\n",
            "Epoch 25300, Loss: 0.22041890025138855\n",
            "Epoch 25400, Loss: 0.21431513130664825\n",
            "Epoch 25500, Loss: 0.20842456817626953\n",
            "Epoch 25600, Loss: 0.20265772938728333\n",
            "Epoch 25700, Loss: 0.19713833928108215\n",
            "Epoch 25800, Loss: 0.19163629412651062\n",
            "Epoch 25900, Loss: 0.1863621324300766\n",
            "Epoch 26000, Loss: 0.18128365278244019\n",
            "Epoch 26100, Loss: 0.17622128129005432\n",
            "Epoch 26200, Loss: 0.17142066359519958\n",
            "Epoch 26300, Loss: 0.16668522357940674\n",
            "Epoch 26400, Loss: 0.1621122658252716\n",
            "Epoch 26500, Loss: 0.15759393572807312\n",
            "Epoch 26600, Loss: 0.15328308939933777\n",
            "Epoch 26700, Loss: 0.14906172454357147\n",
            "Epoch 26800, Loss: 0.14495646953582764\n",
            "Epoch 26900, Loss: 0.14096137881278992\n",
            "Epoch 27000, Loss: 0.1370498389005661\n",
            "Epoch 27100, Loss: 0.1333288997411728\n",
            "Epoch 27200, Loss: 0.12960971891880035\n",
            "Epoch 27300, Loss: 0.1259964108467102\n",
            "Epoch 27400, Loss: 0.1225944459438324\n",
            "Epoch 27500, Loss: 0.11922577768564224\n",
            "Epoch 27600, Loss: 0.11593776196241379\n",
            "Epoch 27700, Loss: 0.11274504661560059\n",
            "Epoch 27800, Loss: 0.1096305400133133\n",
            "Epoch 27900, Loss: 0.1066184714436531\n",
            "Epoch 28000, Loss: 0.10369812697172165\n",
            "Epoch 28100, Loss: 0.10080250352621078\n",
            "Epoch 28200, Loss: 0.09804831445217133\n",
            "Epoch 28300, Loss: 0.09534721076488495\n",
            "Epoch 28400, Loss: 0.09269538521766663\n",
            "Epoch 28500, Loss: 0.09019628167152405\n",
            "Epoch 28600, Loss: 0.08766335248947144\n",
            "Epoch 28700, Loss: 0.0852438360452652\n",
            "Epoch 28800, Loss: 0.08290515840053558\n",
            "Epoch 28900, Loss: 0.08064337074756622\n",
            "Epoch 29000, Loss: 0.07842263579368591\n",
            "Epoch 29100, Loss: 0.07622669637203217\n",
            "Epoch 29200, Loss: 0.07414991408586502\n",
            "Epoch 29300, Loss: 0.07211732864379883\n",
            "Epoch 29400, Loss: 0.0701335072517395\n",
            "Epoch 29500, Loss: 0.06819459050893784\n",
            "Epoch 29600, Loss: 0.06633181124925613\n",
            "Epoch 29700, Loss: 0.06447379291057587\n",
            "Epoch 29800, Loss: 0.06270579248666763\n",
            "Epoch 29900, Loss: 0.06099093332886696\n",
            "Epoch 30000, Loss: 0.05932549387216568\n",
            "Epoch 30100, Loss: 0.0576753206551075\n",
            "Epoch 30200, Loss: 0.05609014257788658\n",
            "Epoch 30300, Loss: 0.05455853417515755\n",
            "Epoch 30400, Loss: 0.05303776264190674\n",
            "Epoch 30500, Loss: 0.05155191570520401\n",
            "Epoch 30600, Loss: 0.05017167702317238\n",
            "Epoch 30700, Loss: 0.048816416412591934\n",
            "Epoch 30800, Loss: 0.047442395240068436\n",
            "Epoch 30900, Loss: 0.046115122735500336\n",
            "Epoch 31000, Loss: 0.044828835874795914\n",
            "Epoch 31100, Loss: 0.043586403131484985\n",
            "Epoch 31200, Loss: 0.042436834424734116\n",
            "Epoch 31300, Loss: 0.04129265248775482\n",
            "Epoch 31400, Loss: 0.04013025015592575\n",
            "Epoch 31500, Loss: 0.039021484553813934\n",
            "Epoch 31600, Loss: 0.0379498228430748\n",
            "Epoch 31700, Loss: 0.03690783679485321\n",
            "Epoch 31800, Loss: 0.0358886644244194\n",
            "Epoch 31900, Loss: 0.034903451800346375\n",
            "Epoch 32000, Loss: 0.033939749002456665\n",
            "Epoch 32100, Loss: 0.033015962690114975\n",
            "Epoch 32200, Loss: 0.03211929649114609\n",
            "Epoch 32300, Loss: 0.03122425451874733\n",
            "Epoch 32400, Loss: 0.030347293242812157\n",
            "Epoch 32500, Loss: 0.029516123235225677\n",
            "Epoch 32600, Loss: 0.0287134051322937\n",
            "Epoch 32700, Loss: 0.02791515737771988\n",
            "Epoch 32800, Loss: 0.027155913412570953\n",
            "Epoch 32900, Loss: 0.026387464255094528\n",
            "Epoch 33000, Loss: 0.025665810331702232\n",
            "Epoch 33100, Loss: 0.024980196729302406\n",
            "Epoch 33200, Loss: 0.024303432554006577\n",
            "Epoch 33300, Loss: 0.023628471419215202\n",
            "Epoch 33400, Loss: 0.022952869534492493\n",
            "Epoch 33500, Loss: 0.02231009304523468\n",
            "Epoch 33600, Loss: 0.021709885448217392\n",
            "Epoch 33700, Loss: 0.02112145535647869\n",
            "Epoch 33800, Loss: 0.020542576909065247\n",
            "Epoch 33900, Loss: 0.019973723217844963\n",
            "Epoch 34000, Loss: 0.019431665539741516\n",
            "Epoch 34100, Loss: 0.01889719069004059\n",
            "Epoch 34200, Loss: 0.018370289355516434\n",
            "Epoch 34300, Loss: 0.01785755157470703\n",
            "Epoch 34400, Loss: 0.017358511686325073\n",
            "Epoch 34500, Loss: 0.016881493851542473\n",
            "Epoch 34600, Loss: 0.01643107831478119\n",
            "Epoch 34700, Loss: 0.015999600291252136\n",
            "Epoch 34800, Loss: 0.01554599404335022\n",
            "Epoch 34900, Loss: 0.015114033594727516\n",
            "Epoch 35000, Loss: 0.01470295898616314\n",
            "Epoch 35100, Loss: 0.01429323572665453\n",
            "Epoch 35200, Loss: 0.013894615694880486\n",
            "Epoch 35300, Loss: 0.01352236419916153\n",
            "Epoch 35400, Loss: 0.013153423555195332\n",
            "Epoch 35500, Loss: 0.012790367007255554\n",
            "Epoch 35600, Loss: 0.012432564049959183\n",
            "Epoch 35700, Loss: 0.012082139030098915\n",
            "Epoch 35800, Loss: 0.011744106188416481\n",
            "Epoch 35900, Loss: 0.011418909765779972\n",
            "Epoch 36000, Loss: 0.011130886152386665\n",
            "Epoch 36100, Loss: 0.010824122466146946\n",
            "Epoch 36200, Loss: 0.010522177442908287\n",
            "Epoch 36300, Loss: 0.010235752910375595\n",
            "Epoch 36400, Loss: 0.009964076802134514\n",
            "Epoch 36500, Loss: 0.009680286049842834\n",
            "Epoch 36600, Loss: 0.00941239483654499\n",
            "Epoch 36700, Loss: 0.0091530941426754\n",
            "Epoch 36800, Loss: 0.008907941170036793\n",
            "Epoch 36900, Loss: 0.008661622181534767\n",
            "Epoch 37000, Loss: 0.008417477831244469\n",
            "Epoch 37100, Loss: 0.008192175067961216\n",
            "Epoch 37200, Loss: 0.007967019453644753\n",
            "Epoch 37300, Loss: 0.007745890412479639\n",
            "Epoch 37400, Loss: 0.00752982497215271\n",
            "Epoch 37500, Loss: 0.007319473195821047\n",
            "Epoch 37600, Loss: 0.007114706095308065\n",
            "Epoch 37700, Loss: 0.006912365555763245\n",
            "Epoch 37800, Loss: 0.006726250983774662\n",
            "Epoch 37900, Loss: 0.00655151903629303\n",
            "Epoch 38000, Loss: 0.0063865892589092255\n",
            "Epoch 38100, Loss: 0.006220461800694466\n",
            "Epoch 38200, Loss: 0.00605451175943017\n",
            "Epoch 38300, Loss: 0.005885525140911341\n",
            "Epoch 38400, Loss: 0.0057163359597325325\n",
            "Epoch 38500, Loss: 0.0055498601868748665\n",
            "Epoch 38600, Loss: 0.005386879667639732\n",
            "Epoch 38700, Loss: 0.005227832123637199\n",
            "Epoch 38800, Loss: 0.005088517442345619\n",
            "Epoch 38900, Loss: 0.0049516502767801285\n",
            "Epoch 39000, Loss: 0.004817047156393528\n",
            "Epoch 39100, Loss: 0.004684679210186005\n",
            "Epoch 39200, Loss: 0.004554415121674538\n",
            "Epoch 39300, Loss: 0.0044268593192100525\n",
            "Epoch 39400, Loss: 0.004302354529500008\n",
            "Epoch 39500, Loss: 0.004183308221399784\n",
            "Epoch 39600, Loss: 0.0040758708491921425\n",
            "Epoch 39700, Loss: 0.003969964571297169\n",
            "Epoch 39800, Loss: 0.0038714143447577953\n",
            "Epoch 39900, Loss: 0.003769415896385908\n",
            "Epoch 40000, Loss: 0.003662874922156334\n",
            "Epoch 40100, Loss: 0.003563770093023777\n",
            "Epoch 40200, Loss: 0.0034668250009417534\n",
            "Epoch 40300, Loss: 0.003372214501723647\n",
            "Epoch 40400, Loss: 0.0032773360144346952\n",
            "Epoch 40500, Loss: 0.003182862652465701\n",
            "Epoch 40600, Loss: 0.003090010955929756\n",
            "Epoch 40700, Loss: 0.0030035036616027355\n",
            "Epoch 40800, Loss: 0.0029197733383625746\n",
            "Epoch 40900, Loss: 0.002842353191226721\n",
            "Epoch 41000, Loss: 0.0027709503192454576\n",
            "Epoch 41100, Loss: 0.0026948361191898584\n",
            "Epoch 41200, Loss: 0.002619304694235325\n",
            "Epoch 41300, Loss: 0.0025450140237808228\n",
            "Epoch 41400, Loss: 0.0024771494790911674\n",
            "Epoch 41500, Loss: 0.0024107652716338634\n",
            "Epoch 41600, Loss: 0.0023453012108802795\n",
            "Epoch 41700, Loss: 0.002280734945088625\n",
            "Epoch 41800, Loss: 0.0022172858007252216\n",
            "Epoch 41900, Loss: 0.0021562506444752216\n",
            "Epoch 42000, Loss: 0.002095987554639578\n",
            "Epoch 42100, Loss: 0.0020369691774249077\n",
            "Epoch 42200, Loss: 0.0019794495310634375\n",
            "Epoch 42300, Loss: 0.0019243094138801098\n",
            "Epoch 42400, Loss: 0.0018765917047858238\n",
            "Epoch 42500, Loss: 0.001828242908231914\n",
            "Epoch 42600, Loss: 0.0017798072658479214\n",
            "Epoch 42700, Loss: 0.001731889322400093\n",
            "Epoch 42800, Loss: 0.0016830572858452797\n",
            "Epoch 42900, Loss: 0.0016340253641828895\n",
            "Epoch 43000, Loss: 0.0015861800638958812\n",
            "Epoch 43100, Loss: 0.0015392948407679796\n",
            "Epoch 43200, Loss: 0.001496525015681982\n",
            "Epoch 43300, Loss: 0.0014600979629904032\n",
            "Epoch 43400, Loss: 0.0014245668426156044\n",
            "Epoch 43500, Loss: 0.0013884076615795493\n",
            "Epoch 43600, Loss: 0.001352308550849557\n",
            "Epoch 43700, Loss: 0.001316408859565854\n",
            "Epoch 43800, Loss: 0.0012806664453819394\n",
            "Epoch 43900, Loss: 0.0012441396247595549\n",
            "Epoch 44000, Loss: 0.0012079766020178795\n",
            "Epoch 44100, Loss: 0.001172493677586317\n",
            "Epoch 44200, Loss: 0.00113784265704453\n",
            "Epoch 44300, Loss: 0.0011057974770665169\n",
            "Epoch 44400, Loss: 0.0010754065588116646\n",
            "Epoch 44500, Loss: 0.0010456955060362816\n",
            "Epoch 44600, Loss: 0.001016975729726255\n",
            "Epoch 44700, Loss: 0.0009888674831017852\n",
            "Epoch 44800, Loss: 0.0009610959095880389\n",
            "Epoch 44900, Loss: 0.0009334924397990108\n",
            "Epoch 45000, Loss: 0.0009059716248884797\n",
            "Epoch 45100, Loss: 0.0008807523408904672\n",
            "Epoch 45200, Loss: 0.0008566789911128581\n",
            "Epoch 45300, Loss: 0.0008329240954481065\n",
            "Epoch 45400, Loss: 0.000809957564342767\n",
            "Epoch 45500, Loss: 0.0007908717379905283\n",
            "Epoch 45600, Loss: 0.0007719234563410282\n",
            "Epoch 45700, Loss: 0.0007525920518673956\n",
            "Epoch 45800, Loss: 0.0007333048270083964\n",
            "Epoch 45900, Loss: 0.0007140388479456306\n",
            "Epoch 46000, Loss: 0.0006950026145204902\n",
            "Epoch 46100, Loss: 0.0006760752876289189\n",
            "Epoch 46200, Loss: 0.000656842952594161\n",
            "Epoch 46300, Loss: 0.0006376891979016364\n",
            "Epoch 46400, Loss: 0.0006191227585077286\n",
            "Epoch 46500, Loss: 0.00060066650621593\n",
            "Epoch 46600, Loss: 0.0005827638087794185\n",
            "Epoch 46700, Loss: 0.0005669503007084131\n",
            "Epoch 46800, Loss: 0.0005515909288078547\n",
            "Epoch 46900, Loss: 0.0005363485543057323\n",
            "Epoch 47000, Loss: 0.0005213632248342037\n",
            "Epoch 47100, Loss: 0.0005069209728389978\n",
            "Epoch 47200, Loss: 0.0004928164416924119\n",
            "Epoch 47300, Loss: 0.00047893839655444026\n",
            "Epoch 47400, Loss: 0.00046563323121517897\n",
            "Epoch 47500, Loss: 0.00045475998194888234\n",
            "Epoch 47600, Loss: 0.00044448100379668176\n",
            "Epoch 47700, Loss: 0.0004331817908678204\n",
            "Epoch 47800, Loss: 0.00042163109173998237\n",
            "Epoch 47900, Loss: 0.0004101446538697928\n",
            "Epoch 48000, Loss: 0.00039880984695628285\n",
            "Epoch 48100, Loss: 0.00038777137524448335\n",
            "Epoch 48200, Loss: 0.00037708086892962456\n",
            "Epoch 48300, Loss: 0.0003664281393866986\n",
            "Epoch 48400, Loss: 0.00035605841549113393\n",
            "Epoch 48500, Loss: 0.0003458260325714946\n",
            "Epoch 48600, Loss: 0.0003358131507411599\n",
            "Epoch 48700, Loss: 0.00032604762236587703\n",
            "Epoch 48800, Loss: 0.00031638366635888815\n",
            "Epoch 48900, Loss: 0.0003069843223784119\n",
            "Epoch 49000, Loss: 0.00029964340501464903\n",
            "Epoch 49100, Loss: 0.0002925547887571156\n",
            "Epoch 49200, Loss: 0.0002852664329111576\n",
            "Epoch 49300, Loss: 0.0002779988572001457\n",
            "Epoch 49400, Loss: 0.00027068101917393506\n",
            "Epoch 49500, Loss: 0.00026352202985435724\n",
            "Epoch 49600, Loss: 0.0002564342867117375\n",
            "Epoch 49700, Loss: 0.0002494549844413996\n",
            "Epoch 49800, Loss: 0.00024253212905023247\n",
            "Epoch 49900, Loss: 0.0002355832839384675\n",
            "\n",
            "Final Filter Weights:\n",
            "[[-1.0591751e+01 -9.9356976e+00 -9.2787552e+00]\n",
            " [ 3.4608468e-01  2.4491544e-03 -3.4117299e-01]\n",
            " [ 9.2833900e+00  9.9390574e+00  1.0595593e+01]]\n",
            "\n",
            "Input Matrix:\n",
            "[[0.02777778 0.05555556 0.08333334 0.11111111 0.1388889  0.16666667]\n",
            " [0.19444445 0.22222222 0.25       0.2777778  0.30555555 0.33333334]\n",
            " [0.3611111  0.3888889  0.41666666 0.44444445 0.4722222  0.5       ]\n",
            " [0.5277778  0.5555556  0.5833333  0.6111111  0.6388889  0.6666667 ]\n",
            " [0.6944444  0.7222222  0.75       0.7777778  0.8055556  0.8333333 ]\n",
            " [0.8611111  0.8888889  0.9166667  0.9444444  0.9722222  1.        ]]\n",
            "\n",
            "Target Matrix:\n",
            "[[10. 10. 10. 10.]\n",
            " [10. 10. 10. 10.]\n",
            " [10. 10. 10. 10.]\n",
            " [10. 10. 10. 10.]]\n",
            "\n",
            "Convolution Result with Trained Filter and ReLU:\n",
            "[[ 9.995472   9.996005   9.996538   9.997072 ]\n",
            " [ 9.9986725  9.999205   9.999739  10.000272 ]\n",
            " [10.001871  10.002406  10.002939  10.003472 ]\n",
            " [10.005072  10.005605  10.00614   10.006671 ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a 6x6 input matrix\n",
        "input_matrix = np.array([\n",
        "    [1, 2, 3, 4, 5, 6],\n",
        "    [7, 8, 9, 10, 11, 12],\n",
        "    [13, 14, 15, 16, 17, 18],\n",
        "    [19, 20, 21, 22, 23, 24],\n",
        "    [25, 26, 27, 28, 29, 30],\n",
        "    [31, 32, 33, 34, 35, 36]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Define a 3x3 filter\n",
        "filter_weights = np.array([\n",
        "    [1, 0, -1],\n",
        "    [2, 0, -2],\n",
        "    [1, 0, -1]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Define a 4x4 target matrix (gradient with respect to the input)\n",
        "target_matrix = np.array([\n",
        "    [10, 10, 10, 10],\n",
        "    [10, 10, 10, 10],\n",
        "    [10, 10, 10, 10],\n",
        "    [10, 10, 10, 10]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "epochs = 50000\n",
        "\n",
        "input_matrix/= np.max(np.abs(input_matrix))\n",
        "\n",
        "# ReLU activation function\n",
        "def relu(x):\n",
        "    return x\n",
        "\n",
        "# Training\n",
        "for epoch in range(epochs):\n",
        "    # Perform convolution\n",
        "    conv_result = np.zeros((4, 4), dtype=np.float32)\n",
        "    for i in range(conv_result.shape[0]):\n",
        "        for j in range(conv_result.shape[1]):\n",
        "            conv_result[i, j] = (np.sum(input_matrix[i:i+filter_weights.shape[0], j:j+filter_weights.shape[1]] * filter_weights))\n",
        "\n",
        "    # Calculate the error\n",
        "    error = target_matrix - conv_result\n",
        "\n",
        "    listGradient=[]\n",
        "    # Calculate the gradient\n",
        "    gradient = np.zeros_like(filter_weights, dtype=np.float32)\n",
        "    for i in range(gradient.shape[0]):\n",
        "        for j in range(gradient.shape[1]):\n",
        "            e = error[i,j]\n",
        "            # print(e)\n",
        "            gradient = e * -1 * 1 * input_matrix[i:i+filter_weights.shape[0], j:j+filter_weights.shape[1]]\n",
        "            # gradient /= np.max(np.abs(gradient))\n",
        "            listGradient.append(gradient)\n",
        "            # print(gradient)\n",
        "\n",
        "    # Normalize the gradient\n",
        "    # gradient /= np.max(np.abs(gradient))\n",
        "\n",
        "    gradient=np.sum(listGradient, axis=0)\n",
        "\n",
        "    # Update the filter weights using gradient descent\n",
        "    filter_weights = filter_weights - learning_rate * gradient\n",
        "\n",
        "    # Print the loss every 10 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        loss = np.sum(error ** 2)\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Print the final filter weights\n",
        "print(\"\\nFinal Filter Weights:\")\n",
        "print(filter_weights)\n",
        "\n",
        "# Perform convolution with the trained filter and ReLU activation\n",
        "conv_result_relu = np.zeros((4, 4), dtype=np.float32)\n",
        "for i in range(conv_result_relu.shape[0]):\n",
        "    for j in range(conv_result_relu.shape[1]):\n",
        "        conv_result_relu[i, j] = relu(np.sum(input_matrix[i:i+filter_weights.shape[0], j:j+filter_weights.shape[1]] * filter_weights))\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nInput Matrix:\")\n",
        "print(input_matrix)\n",
        "print(\"\\nTarget Matrix:\")\n",
        "print(target_matrix)\n",
        "print(\"\\nConvolution Result with Trained Filter and ReLU:\")\n",
        "print(conv_result_relu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlNeVeeksq5X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a 5x5 matrix as input\n",
        "input_matrix = np.array([\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [6, 7, 8, 9, 10],\n",
        "    [11, 12, 13, 14, 15],\n",
        "    [16, 17, 18, 19, 20],\n",
        "    [21, 22, 23, 24, 25]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Define a 3x3 filter\n",
        "filter_weights = np.array([\n",
        "    [1, 0, -1],\n",
        "    [2, 0, -2],\n",
        "    [1, 0, -1]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Define a target matrix (gradients with respect to the input)\n",
        "target_matrix = np.array([\n",
        "    [10, 10, 10],\n",
        "    [10, 10, 10],\n",
        "    [10, 10, 10]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "epochs = 10000\n",
        "\n",
        "# ReLU activation function\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Perform gradient descent with ReLU activation\n",
        "for epoch in range(epochs):\n",
        "    # Perform convolution\n",
        "    conv_result = np.zeros_like(target_matrix, dtype=np.float32)\n",
        "    for i in range(conv_result.shape[0]):\n",
        "        for j in range(conv_result.shape[1]):\n",
        "            conv_result[i, j] = relu(np.sum(input_matrix[i:i+filter_weights.shape[0], j:j+filter_weights.shape[1]] * filter_weights))\n",
        "\n",
        "    # Calculate the error\n",
        "    error = conv_result - target_matrix\n",
        "\n",
        "    # Calculate the gradient\n",
        "    gradient = np.zeros_like(filter_weights, dtype=np.float32)\n",
        "    for i in range(gradient.shape[0]):\n",
        "        for j in range(gradient.shape[1]):\n",
        "            gradient[i, j] = np.sum(error * input_matrix[i:i+error.shape[0], j:j+error.shape[1]])\n",
        "\n",
        "    # Normalize the gradient\n",
        "    gradient /= np.max(np.abs(gradient))\n",
        "\n",
        "    # Update the filter weights using gradient descent\n",
        "    filter_weights -= learning_rate * gradient\n",
        "\n",
        "    # Print the loss every 10 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.sum(error ** 2)\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Print the final filter weights\n",
        "print(\"\\nFinal Filter Weights:\")\n",
        "print(filter_weights)\n",
        "\n",
        "# Perform convolution with the trained filter and ReLU activation on the original input_matrix\n",
        "conv_result_original_relu = np.zeros_like(target_matrix, dtype=np.float32)\n",
        "for i in range(conv_result_original_relu.shape[0]):\n",
        "    for j in range(conv_result_original_relu.shape[1]):\n",
        "        conv_result_original_relu[i, j] = relu(np.sum(input_matrix[i:i+filter_weights.shape[0], j:j+filter_weights.shape[1]] * filter_weights))\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nOriginal Input Matrix:\")\n",
        "print(input_matrix)\n",
        "print(\"\\nConvolution Result with Trained Filter and ReLU on Original Input Matrix:\")\n",
        "print(conv_result_original_relu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_gUZeUYtxsd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define two 5x5 matrices as input\n",
        "input_matrix1 = np.array([\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [6, 7, 8, 9, 10],\n",
        "    [11, 12, 13, 14, 15],\n",
        "    [16, 17, 18, 19, 20],\n",
        "    [21, 22, 23, 24, 25]\n",
        "], dtype=np.float32)\n",
        "\n",
        "input_matrix2 = np.array([\n",
        "    [5, 4, 3, 2, 1],\n",
        "    [10, 9, 8, 7, 6],\n",
        "    [15, 14, 13, 12, 11],\n",
        "    [20, 19, 18, 17, 16],\n",
        "    [25, 24, 23, 22, 21]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Define a 3x3 filter\n",
        "filter_weights = np.array([\n",
        "    [1, 0, -1],\n",
        "    [2, 0, -2],\n",
        "    [1, 0, -1]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Define two target matrices (gradients with respect to the input)\n",
        "target_matrix1 = np.array([\n",
        "    [10, 10, 10],\n",
        "    [10, 10, 10],\n",
        "    [10, 10, 10]\n",
        "], dtype=np.float32)\n",
        "\n",
        "target_matrix2 = np.array([\n",
        "    [5, 5, 5],\n",
        "    [5, 5, 5],\n",
        "    [5, 5, 5]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "epochs = 30000\n",
        "\n",
        "# ReLU activation function\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Training with input_matrix1 and target_matrix1\n",
        "for epoch in range(epochs):\n",
        "    # Perform convolution\n",
        "    conv_result1 = np.zeros_like(target_matrix1, dtype=np.float32)\n",
        "    for i in range(conv_result1.shape[0]):\n",
        "        for j in range(conv_result1.shape[1]):\n",
        "            conv_result1[i, j] = relu(np.sum(input_matrix1[i:i+filter_weights.shape[0], j:j+filter_weights.shape[1]] * filter_weights))\n",
        "\n",
        "    # Calculate the error\n",
        "    error1 = conv_result1 - target_matrix1\n",
        "\n",
        "    # Calculate the gradient\n",
        "    gradient1 = np.zeros_like(filter_weights, dtype=np.float32)\n",
        "    for i in range(gradient1.shape[0]):\n",
        "        for j in range(gradient1.shape[1]):\n",
        "            gradient1[i, j] = np.sum(error1 * input_matrix1[i:i+error1.shape[0], j:j+error1.shape[1]])\n",
        "\n",
        "    # Normalize the gradient\n",
        "    gradient1 /= np.max(np.abs(gradient1))\n",
        "\n",
        "    # Update the filter weights using gradient descent\n",
        "    filter_weights -= learning_rate * gradient1\n",
        "\n",
        "    # Perform convolution\n",
        "    conv_result2 = np.zeros_like(target_matrix2, dtype=np.float32)\n",
        "    for i in range(conv_result2.shape[0]):\n",
        "        for j in range(conv_result2.shape[1]):\n",
        "            conv_result2[i, j] = relu(np.sum(input_matrix2[i:i+filter_weights.shape[0], j:j+filter_weights.shape[1]] * filter_weights))\n",
        "\n",
        "    # Calculate the error\n",
        "    error2 = conv_result2 - target_matrix2\n",
        "\n",
        "    # Calculate the gradient\n",
        "    gradient2 = np.zeros_like(filter_weights, dtype=np.float32)\n",
        "    for i in range(gradient2.shape[0]):\n",
        "        for j in range(gradient2.shape[1]):\n",
        "            gradient2[i, j] = np.sum(error2 * input_matrix2[i:i+error2.shape[0], j:j+error2.shape[1]])\n",
        "\n",
        "    # Normalize the gradient\n",
        "    gradient2 /= np.max(np.abs(gradient2))\n",
        "\n",
        "    # Update the filter weights using gradient descent\n",
        "    filter_weights -= learning_rate * gradient2\n",
        "\n",
        "    # Print the loss every 10 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        loss2 = np.sum(error2 ** 2)+np.sum(error1 ** 2)\n",
        "        print(f\"Epoch {epoch} (Input 2, Target 2), Loss: {loss2}\")\n",
        "\n",
        "# Print the final filter weights\n",
        "print(\"\\nFinal Filter Weights:\")\n",
        "print(filter_weights)\n",
        "\n",
        "# Perform convolution with the trained filter and ReLU activation on input_matrix1 and target_matrix1\n",
        "conv_result1_relu = np.zeros_like(target_matrix1, dtype=np.float32)\n",
        "for i in range(conv_result1_relu.shape[0]):\n",
        "    for j in range(conv_result1_relu.shape[1]):\n",
        "        conv_result1_relu[i, j] = relu(np.sum(input_matrix1[i:i+filter_weights.shape[0], j:j+filter_weights.shape[1]] * filter_weights))\n",
        "\n",
        "# Perform convolution with the trained filter and ReLU activation on input_matrix2 and target_matrix2\n",
        "conv_result2_relu = np.zeros_like(target_matrix2, dtype=np.float32)\n",
        "for i in range(conv_result2_relu.shape[0]):\n",
        "    for j in range(conv_result2_relu.shape[1]):\n",
        "        conv_result2_relu[i, j] = relu(np.sum(input_matrix2[i:i+filter_weights.shape[0], j:j+filter_weights.shape[1]] * filter_weights))\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nInput Matrix 1:\")\n",
        "print(input_matrix1)\n",
        "print(\"\\nConvolution Result with Trained Filter and ReLU on Input Matrix 1:\")\n",
        "print(conv_result1_relu)\n",
        "\n",
        "print(\"\\nInput Matrix 2:\")\n",
        "print(input_matrix2)\n",
        "print(\"\\nConvolution Result with Trained Filter and ReLU on Input Matrix 2:\")\n",
        "print(conv_result2_relu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzxQK540v_jL",
        "outputId": "229650a3-4f2e-46e8-d9f1-f06ab197a785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Average Loss: 394.31129201253253\n",
            "Epoch 1000, Average Loss: 106.54151980082194\n",
            "Epoch 2000, Average Loss: 83.59505716959636\n",
            "Epoch 3000, Average Loss: 84.86994043986003\n",
            "Epoch 4000, Average Loss: 84.72880045572917\n",
            "Epoch 5000, Average Loss: 83.16801007588704\n",
            "Epoch 6000, Average Loss: 81.09403165181477\n",
            "Epoch 7000, Average Loss: 78.89711888631184\n",
            "Epoch 8000, Average Loss: 76.7125924428304\n",
            "Epoch 9000, Average Loss: 74.58285013834636\n",
            "Epoch 10000, Average Loss: 72.52041943868001\n",
            "Epoch 11000, Average Loss: 70.52784538269043\n",
            "Epoch 12000, Average Loss: 68.60388310750325\n",
            "Epoch 13000, Average Loss: 66.7471211751302\n",
            "Epoch 14000, Average Loss: 64.95509020487468\n",
            "Epoch 15000, Average Loss: 63.22580528259277\n",
            "Epoch 16000, Average Loss: 61.557067235310875\n",
            "Epoch 17000, Average Loss: 59.94713020324707\n",
            "Epoch 18000, Average Loss: 58.39397048950195\n",
            "Epoch 19000, Average Loss: 56.89552625020345\n",
            "\n",
            "Final Filter Weights:\n",
            "[[-2.7085752  -1.8693738  -1.0301826 ]\n",
            " [ 1.1135106   0.9527454   0.79195076]\n",
            " [ 2.9356682   3.7748718   4.614074  ]]\n",
            "\n",
            "Input Matrix 1:\n",
            "[[0.         0.04166667 0.08333334 0.125      0.16666667]\n",
            " [0.20833333 0.25       0.29166666 0.33333334 0.375     ]\n",
            " [0.41666666 0.45833334 0.5        0.5416667  0.5833333 ]\n",
            " [0.625      0.6666667  0.7083333  0.75       0.7916667 ]\n",
            " [0.8333333  0.875      0.9166667  0.9583333  1.        ]]\n",
            "Convolution Result with Trained Filter and ReLU on Input Matrix 1:\n",
            "[[ 5.7977962  6.155075   6.512354 ]\n",
            " [ 7.584189   7.941468   8.298747 ]\n",
            " [ 9.370584   9.727861  10.08514  ]]\n",
            "\n",
            "Input Matrix 2:\n",
            "[[0.16666667 0.125      0.08333334 0.04166667 0.        ]\n",
            " [0.375      0.33333334 0.29166666 0.25       0.20833333]\n",
            " [0.5833333  0.5416667  0.5        0.45833334 0.41666666]\n",
            " [0.7916667  0.75       0.7083333  0.6666667  0.625     ]\n",
            " [1.         0.9583333  0.9166667  0.875      0.8333333 ]]\n",
            "Convolution Result with Trained Filter and ReLU on Input Matrix 2:\n",
            "[[6.2594166 5.902138  5.5448594]\n",
            " [8.045811  7.688532  7.331253 ]\n",
            " [9.832204  9.474926  9.117647 ]]\n",
            "\n",
            "Input Matrix 3:\n",
            "[[0.         0.         0.         0.         0.        ]\n",
            " [0.04166667 0.04166667 0.04166667 0.04166667 0.04166667]\n",
            " [0.08333334 0.08333334 0.08333334 0.08333334 0.08333334]\n",
            " [0.125      0.125      0.125      0.125      0.125     ]\n",
            " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]]\n",
            "Convolution Result with Trained Filter and ReLU on Input Matrix 3:\n",
            "[[1.0628098 1.0628098 1.0628098]\n",
            " [1.4200885 1.4200885 1.4200885]\n",
            " [1.7773674 1.7773674 1.7773674]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define five 5x5 matrices as input\n",
        "input_matrix1 = np.array([\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [6, 7, 8, 9, 10],\n",
        "    [11, 12, 13, 14, 15],\n",
        "    [16, 17, 18, 19, 20],\n",
        "    [21, 22, 23, 24, 25]\n",
        "], dtype=np.float32)\n",
        "\n",
        "input_matrix2 = np.array([\n",
        "    [5, 4, 3, 2, 1],\n",
        "    [10, 9, 8, 7, 6],\n",
        "    [15, 14, 13, 12, 11],\n",
        "    [20, 19, 18, 17, 16],\n",
        "    [25, 24, 23, 22, 21]\n",
        "], dtype=np.float32)\n",
        "\n",
        "input_matrix3 = np.array([\n",
        "    [1, 1, 1, 1, 1],\n",
        "    [2, 2, 2, 2, 2],\n",
        "    [3, 3, 3, 3, 3],\n",
        "    [4, 4, 4, 4, 4],\n",
        "    [5, 5, 5, 5, 5]\n",
        "], dtype=np.float32)\n",
        "\n",
        "\n",
        "# Concatenate input matrices into an array\n",
        "input_matrices = np.array([input_matrix1, input_matrix2, input_matrix3])\n",
        "\n",
        "# Define a 3x3 filter\n",
        "filter_weights = np.array([\n",
        "    [1, 0, -1],\n",
        "    [2, 0, -2],\n",
        "    [1, 0, -1]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Define five target matrices (gradients with respect to the input)\n",
        "target_matrix1 = np.array([\n",
        "    [10, 10, 10],\n",
        "    [10, 10, 10],\n",
        "    [10, 10, 10]\n",
        "], dtype=np.float32)\n",
        "\n",
        "target_matrix2 = np.array([\n",
        "    [5, 5, 5],\n",
        "    [5, 5, 5],\n",
        "    [5, 5, 5]\n",
        "], dtype=np.float32)\n",
        "\n",
        "target_matrix3 = np.array([\n",
        "    [3, 3, 3],\n",
        "    [3, 3, 3],\n",
        "    [3, 3, 3]\n",
        "], dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "# Concatenate target matrices into an array\n",
        "target_matrices = np.array([target_matrix1, target_matrix2, target_matrix3])\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "epochs = 150000\n",
        "\n",
        "# ReLU activation function\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Training for all input-target pairs\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    # Iterate over all input-target pairs\n",
        "    for i in range(len(input_matrices)):\n",
        "        # Get the current input and target matrices\n",
        "        current_input_matrix = input_matrices[i]\n",
        "        current_target_matrix = target_matrices[i]\n",
        "\n",
        "        # Perform convolution\n",
        "        conv_result = np.zeros_like(current_target_matrix, dtype=np.float32)\n",
        "        for j in range(conv_result.shape[0]):\n",
        "            for k in range(conv_result.shape[1]):\n",
        "                conv_result[j, k] = relu(np.sum(current_input_matrix[j:j+filter_weights.shape[0], k:k+filter_weights.shape[1]] * filter_weights))\n",
        "\n",
        "        # Calculate the error\n",
        "        error = conv_result - current_target_matrix\n",
        "\n",
        "        # Calculate the gradient\n",
        "        gradient = np.zeros_like(filter_weights, dtype=np.float32)\n",
        "        for j in range(gradient.shape[0]):\n",
        "            for k in range(gradient.shape[1]):\n",
        "                gradient[j, k] = np.sum(error * current_input_matrix[j:j+error.shape[0], k:k+error.shape[1]])\n",
        "\n",
        "        # Normalize the gradient\n",
        "        gradient /= np.max(np.abs(gradient))\n",
        "\n",
        "        # Update the filter weights using gradient descent\n",
        "        filter_weights -= learning_rate * gradient\n",
        "\n",
        "        # Accumulate the loss\n",
        "        total_loss += np.sum(error ** 2)\n",
        "\n",
        "    # Print the average loss every 10 epochs\n",
        "    if epoch % 3000 == 0:\n",
        "        average_loss = total_loss / len(input_matrices)\n",
        "        print(f\"Epoch {epoch}, Average Loss: {average_loss}\")\n",
        "\n",
        "# Print the final filter weights\n",
        "print(\"\\nFinal Filter Weights:\")\n",
        "print(filter_weights)\n",
        "\n",
        "# Perform convolution with the trained filter and ReLU activation on all input-target pairs\n",
        "for i in range(len(input_matrices)):\n",
        "    current_input_matrix = input_matrices[i]\n",
        "    current_target_matrix = target_matrices[i]\n",
        "\n",
        "    conv_result_relu = np.zeros_like(current_target_matrix, dtype=np.float32)\n",
        "    for j in range(conv_result_relu.shape[0]):\n",
        "        for k in range(conv_result_relu.shape[1]):\n",
        "            conv_result_relu[j, k] = relu(np.sum(current_input_matrix[j:j+filter_weights.shape[0], k:k+filter_weights.shape[1]] * filter_weights))\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"\\nInput Matrix {i+1}:\")\n",
        "    print(current_input_matrix)\n",
        "    print(f\"Convolution Result with Trained Filter and ReLU on Input Matrix {i+1}:\")\n",
        "    print(conv_result_relu)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}